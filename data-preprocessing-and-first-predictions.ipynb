{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d92f96",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-23T13:46:30.890324Z",
     "iopub.status.busy": "2023-03-23T13:46:30.889326Z",
     "iopub.status.idle": "2023-03-23T13:46:42.261552Z",
     "shell.execute_reply": "2023-03-23T13:46:42.260354Z"
    },
    "papermill": {
     "duration": 11.381474,
     "end_time": "2023-03-23T13:46:42.264086",
     "exception": false,
     "start_time": "2023-03-23T13:46:30.882612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7947076",
   "metadata": {
    "papermill": {
     "duration": 0.003645,
     "end_time": "2023-03-23T13:46:42.271970",
     "exception": false,
     "start_time": "2023-03-23T13:46:42.268325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921b90a",
   "metadata": {
    "papermill": {
     "duration": 0.003539,
     "end_time": "2023-03-23T13:46:42.279429",
     "exception": false,
     "start_time": "2023-03-23T13:46:42.275890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Data Preprocessing part 1: \n",
    "1. Drop Columns: salary_range, department. <br>\n",
    "2. Add country, state for each entry and calculate percentage of fake jobs based on state. \n",
    "3. Fill the NaN values with blank spaces in the textual \n",
    "4. Create a text column with all the textual categories and drop them from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018398bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:46:42.289895Z",
     "iopub.status.busy": "2023-03-23T13:46:42.288434Z",
     "iopub.status.idle": "2023-03-23T13:46:43.828353Z",
     "shell.execute_reply": "2023-03-23T13:46:43.826681Z"
    },
    "papermill": {
     "duration": 1.548028,
     "end_time": "2023-03-23T13:46:43.831124",
     "exception": false,
     "start_time": "2023-03-23T13:46:42.283096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv\")\n",
    "data.drop([\"salary_range\", \"department\"], axis='columns', inplace = True)\n",
    "\n",
    "#function to  get the country where the job is posted \n",
    "def country(text):\n",
    "    if type(text) != float: #location is not null\n",
    "         return text.split(',')[0]\n",
    "    else: return ' '\n",
    "\n",
    "#adding a column to the original dataset with the country where the job is posted    \n",
    "data['country'] = data.location.apply(country)\n",
    "\n",
    "# Creating a new dataset with jobs posted only in the US\n",
    "data_US = data[data[\"country\"] == 'US']\n",
    "data_US = data_US.reset_index()\n",
    "data_US.drop('index', axis = 'columns', inplace = True)\n",
    "\n",
    "#Adding a column that indicates the state where the job was posted\n",
    "def state(text):\n",
    "    if len(text) > 3: return text.split(',')[1]\n",
    "    else: return ' '    \n",
    "\n",
    "data_US['state'] = data_US.location.apply(state)\n",
    "#Creating two seperate datasets for real jobs and fake jobs in the US\n",
    "data_US_fake = data_US[data_US['fraudulent'] == 1]\n",
    "# Creating a new attribute, for each state we calculate the pecentage of fake jobs\n",
    "state_df = data_US.state.value_counts().to_frame().rename(columns = {'state' : 'no of jobs'})\n",
    "state_df['no of fake jobs'] = data_US_fake.state.value_counts()\n",
    "state_df['p_fake_jobs'] = (state_df['no of fake jobs'] / state_df['no of jobs'])\n",
    "#adding a new column to the US dataset, a percentage of fake jobs \n",
    "state_df.drop(' ', axis = 'index', inplace = True)\n",
    "states_percentage = state_df['p_fake_jobs'].to_dict()  #creating a dictionary with each state and the percentage of fake jobs to add the  value to the dataset\n",
    "data_US['percentage of fake jobs'] = data_US['state'].map(states_percentage)\n",
    "\n",
    "#Creating a column with all the textual data\n",
    "data_US_text = data_US[['title', 'location', 'company_profile', 'description', 'requirements', 'benefits',\n",
    "                        'employment_type', 'required_experience','required_education', 'industry', 'function', 'fraudulent']]\n",
    "data_US_text = data_US_text.fillna(' ')\n",
    "data_US_text['text'] = data_US_text['title'] + ' ' + data_US_text['location'] + ' ' + data_US_text['company_profile'] + ' '+ data_US_text['description'] + ' '+ data_US_text['requirements'] + ' '+ data_US_text['benefits'] + ' '+ data_US_text['employment_type'] + ' '+ data_US_text['required_experience'] + ' ' + data_US_text['required_education'] + ' '+ data_US_text['industry'] + ' ' + data_US_text['function']\n",
    "data_US_text.drop(columns = ['title', 'location', 'company_profile', 'description', 'requirements', 'benefits',\n",
    "                        'employment_type', 'required_experience','required_education', 'industry', 'function' ], inplace = True)\n",
    "\n",
    "data_US_text_real = data_US_text[data_US_text['fraudulent'] == 0]\n",
    "data_US_text_fake = data_US_text[data_US_text['fraudulent'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73405f4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T12:39:26.419101Z",
     "iopub.status.busy": "2023-03-15T12:39:26.418659Z",
     "iopub.status.idle": "2023-03-15T12:39:26.435247Z",
     "shell.execute_reply": "2023-03-15T12:39:26.433732Z",
     "shell.execute_reply.started": "2023-03-15T12:39:26.419063Z"
    },
    "papermill": {
     "duration": 0.003875,
     "end_time": "2023-03-23T13:46:43.839525",
     "exception": false,
     "start_time": "2023-03-23T13:46:43.835650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Data Preprocessing part 2: \n",
    "1. Convert to lower case.\n",
    "2. Clean text from punctuation, numbers, links (https), symbols etc.\n",
    "3. Clean text from stopwords.\n",
    "4. Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c988d14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:46:43.849911Z",
     "iopub.status.busy": "2023-03-23T13:46:43.849318Z",
     "iopub.status.idle": "2023-03-23T13:48:09.958691Z",
     "shell.execute_reply": "2023-03-23T13:48:09.956716Z"
    },
    "papermill": {
     "duration": 86.117664,
     "end_time": "2023-03-23T13:48:09.961596",
     "exception": false,
     "start_time": "2023-03-23T13:46:43.843932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(row):\n",
    "    t = row['text']\n",
    "    #Lower case\n",
    "    t = t.lower()\n",
    "    #Removing punctuation, links, numbers, _/-/@/% etc. \n",
    "    t = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|[0-9]|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", t)\n",
    "    #Removing the extra spaces created \n",
    "    t = re.sub(' +', ' ', t)\n",
    "    #English stopwords are removed\n",
    "    fulltext = t.split()\n",
    "    stop = stopwords.words('english')\n",
    "    t = \" \".join([w for w in fulltext if w not in (stop)])\n",
    "    #Stemming\n",
    "    fulltext = t.split()\n",
    "    fulltext_stem = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for w in fulltext:\n",
    "        fulltext_stem.append(stemmer.stem(w))\n",
    "\n",
    "    return ' '.join(s for s in fulltext_stem)\n",
    "\n",
    "data_US_text['clean_text'] = data_US_text.apply(clean_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a129f16a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:48:09.972438Z",
     "iopub.status.busy": "2023-03-23T13:48:09.971895Z",
     "iopub.status.idle": "2023-03-23T13:48:10.745172Z",
     "shell.execute_reply": "2023-03-23T13:48:10.744008Z"
    },
    "papermill": {
     "duration": 0.782272,
     "end_time": "2023-03-23T13:48:10.748242",
     "exception": false,
     "start_time": "2023-03-23T13:48:09.965970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_US_text.to_csv('data_US_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f74f73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:48:10.758697Z",
     "iopub.status.busy": "2023-03-23T13:48:10.758288Z",
     "iopub.status.idle": "2023-03-23T13:48:11.264506Z",
     "shell.execute_reply": "2023-03-23T13:48:11.262920Z"
    },
    "papermill": {
     "duration": 0.51606,
     "end_time": "2023-03-23T13:48:11.268627",
     "exception": false,
     "start_time": "2023-03-23T13:48:10.752567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_b = pd.read_csv(\"/kaggle/working/data_US_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95bf41d",
   "metadata": {
    "papermill": {
     "duration": 0.003563,
     "end_time": "2023-03-23T13:48:11.277134",
     "exception": false,
     "start_time": "2023-03-23T13:48:11.273571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Converting to numerical form and spitting train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b93d9b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:48:11.289687Z",
     "iopub.status.busy": "2023-03-23T13:48:11.288773Z",
     "iopub.status.idle": "2023-03-23T13:48:33.356791Z",
     "shell.execute_reply": "2023-03-23T13:48:33.355240Z"
    },
    "papermill": {
     "duration": 22.077227,
     "end_time": "2023-03-23T13:48:33.359206",
     "exception": false,
     "start_time": "2023-03-23T13:48:11.281979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7459, 93400)\n",
      "(3197, 93400)\n",
      "(7459,)\n",
      "(3197,)\n"
     ]
    }
   ],
   "source": [
    "def ready_for_training(text, y, ngram, bbool):\n",
    "    #Converting the textual data into numerical form to feed to the predicting models, using the Bag of Words approach\n",
    "    vec = CountVectorizer(ngram_range = ngram, binary = bbool)\n",
    "    X = vec.fit_transform(text).toarray() #X = vec.fit_transform(data_US_text['clean_text']).toarray()\n",
    "    #The BoW approach gives each word a score based on its occurence in the text, but does not take into consideration how\n",
    "    #frequent this word is in all the texts, thus I use TF-IDF that considers the all the texts to assign a weightage to a word.\n",
    "    tfidf = TfidfTransformer()\n",
    "    X = tfidf.fit_transform(X).toarray()    \n",
    "    return train_test_split(X, y, test_size=0.3, random_state=0) #splitting to train/test\n",
    "\n",
    "X_train, X_test, y_train, y_test = ready_for_training(data_b['clean_text'], data_b.fraudulent,(1,1), False)\n",
    "#considering bigrams (1,2) the session crashes. \n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36537fd",
   "metadata": {
    "papermill": {
     "duration": 0.006065,
     "end_time": "2023-03-23T13:48:33.369680",
     "exception": false,
     "start_time": "2023-03-23T13:48:33.363615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47096273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:48:33.381570Z",
     "iopub.status.busy": "2023-03-23T13:48:33.381110Z",
     "iopub.status.idle": "2023-03-23T13:48:33.388809Z",
     "shell.execute_reply": "2023-03-23T13:48:33.386992Z"
    },
    "papermill": {
     "duration": 0.017102,
     "end_time": "2023-03-23T13:48:33.391972",
     "exception": false,
     "start_time": "2023-03-23T13:48:33.374870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predictions(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    models = [LogisticRegression(), RandomForestClassifier()]\n",
    "    for classifier in models:\n",
    "        print('Model used: ' + str(classifier))\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0a4fe86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:48:33.402676Z",
     "iopub.status.busy": "2023-03-23T13:48:33.402200Z",
     "iopub.status.idle": "2023-03-23T13:51:18.548236Z",
     "shell.execute_reply": "2023-03-23T13:51:18.546842Z"
    },
    "papermill": {
     "duration": 165.157909,
     "end_time": "2023-03-23T13:51:18.554182",
     "exception": false,
     "start_time": "2023-03-23T13:48:33.396273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used: LogisticRegression()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      2998\n",
      "           1       1.00      0.40      0.57       199\n",
      "\n",
      "    accuracy                           0.96      3197\n",
      "   macro avg       0.98      0.70      0.78      3197\n",
      "weighted avg       0.96      0.96      0.96      3197\n",
      "\n",
      "Model used: RandomForestClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      2998\n",
      "           1       1.00      0.60      0.75       199\n",
      "\n",
      "    accuracy                           0.98      3197\n",
      "   macro avg       0.99      0.80      0.87      3197\n",
      "weighted avg       0.98      0.98      0.97      3197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83cfaf80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:51:18.563981Z",
     "iopub.status.busy": "2023-03-23T13:51:18.563602Z",
     "iopub.status.idle": "2023-03-23T13:54:14.194276Z",
     "shell.execute_reply": "2023-03-23T13:54:14.192508Z"
    },
    "papermill": {
     "duration": 175.638909,
     "end_time": "2023-03-23T13:54:14.197060",
     "exception": false,
     "start_time": "2023-03-23T13:51:18.558151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                4670050   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                1530      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 15)                465       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,672,061\n",
      "Trainable params: 4,672,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "597/597 [==============================] - 18s 28ms/step - loss: 0.1669 - accuracy: 0.9435 - val_loss: 0.0702 - val_accuracy: 0.9799\n",
      "Epoch 2/10\n",
      "597/597 [==============================] - 18s 30ms/step - loss: 0.0228 - accuracy: 0.9925 - val_loss: 0.0495 - val_accuracy: 0.9792\n",
      "Epoch 3/10\n",
      "597/597 [==============================] - 18s 30ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0627 - val_accuracy: 0.9826\n",
      "Epoch 4/10\n",
      "597/597 [==============================] - 18s 30ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0511 - val_accuracy: 0.9832\n",
      "Epoch 5/10\n",
      "597/597 [==============================] - 17s 29ms/step - loss: 3.1413e-04 - accuracy: 1.0000 - val_loss: 0.0520 - val_accuracy: 0.9826\n",
      "Epoch 6/10\n",
      "597/597 [==============================] - 18s 30ms/step - loss: 2.3273e-04 - accuracy: 1.0000 - val_loss: 0.0591 - val_accuracy: 0.9812\n",
      "Epoch 7/10\n",
      "597/597 [==============================] - 17s 29ms/step - loss: 7.9228e-04 - accuracy: 0.9998 - val_loss: 0.0568 - val_accuracy: 0.9826\n",
      "Epoch 8/10\n",
      "597/597 [==============================] - 17s 28ms/step - loss: 5.3689e-05 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 0.9819\n",
      "Epoch 9/10\n",
      "597/597 [==============================] - 17s 28ms/step - loss: 2.6082e-05 - accuracy: 1.0000 - val_loss: 0.0623 - val_accuracy: 0.9819\n",
      "Epoch 10/10\n",
      "597/597 [==============================] - 16s 27ms/step - loss: 1.6011e-05 - accuracy: 1.0000 - val_loss: 0.0639 - val_accuracy: 0.9819\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape=(X_train.shape[1],), activation='relu')) \n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary() \n",
    "\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "\n",
    "#no neeed for early stopping, no of epoch small\n",
    "history = model.fit(X_train,y_train,\n",
    "                    epochs=10, \n",
    "                    batch_size=10,\n",
    "                    validation_split=0.2,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6274445d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:54:14.512876Z",
     "iopub.status.busy": "2023-03-23T13:54:14.512147Z",
     "iopub.status.idle": "2023-03-23T13:54:16.919721Z",
     "shell.execute_reply": "2023-03-23T13:54:16.917494Z"
    },
    "papermill": {
     "duration": 2.567957,
     "end_time": "2023-03-23T13:54:16.922930",
     "exception": false,
     "start_time": "2023-03-23T13:54:14.354973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 10ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2998\n",
      "           1       0.92      0.82      0.86       199\n",
      "\n",
      "    accuracy                           0.98      3197\n",
      "   macro avg       0.95      0.91      0.93      3197\n",
      "weighted avg       0.98      0.98      0.98      3197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_test),0)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 478.890962,
   "end_time": "2023-03-23T13:54:19.729661",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-23T13:46:20.838699",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
